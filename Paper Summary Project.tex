\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\title{\textbf{GhostWriter: Using an LSTM for Automatic Rap Lyric Generation Summary}}
\author{Tanish Tyagi}
\date{February 17, 2022}

\begin{document}

\maketitle

\begin{item}
\section{Identifiers}
\item \textbf{Citation:} Potash, P., Romanov, A., & Rumshisky, A. (2015, September). Ghostwriter: Using an lstm for automatic rap lyric generation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 1919-1924).
\item \textbf{Url:} https://aclanthology.org/D15-1221.pdf
\end{item}

\begin{item}
\section{General Information}

\item \textbf{Key Words:} The paper does not formally list keywords, but based on abstract, appropriate keywords would be lyric generation and LSTM. 

\item \textbf{General Subject:} The general premise of this paper was to generate rap lyrics that mimic the style of a given rapper with unique lyrics. Essentially, automate the task of ghostwriting, hence the name of the title.

\item \textbf{Specific Subject:} To automate the task of ghostwriting, the authors of the paper sought to create a Long Short-Term Memory (LSTM) model that would define its own rhyme scheme, and line and verse length. 

\end{item}

\section{Related Works}
Recent work has shown the effectiveness of Recurrent Neural Networks (RNN) for text generation. Previous works have been able to learn various grammatical and punctuation rules and a large vocabulary of English words at the character level. In regards to text generation, previous works have generated fixed 16-line verses using full lines from existing rap songs. This work is novel in the fact that it is able to more aptly reflect an artist's style.

\begin{item}
\section{Methods}
\item \textbf{Dataset Generation:} The authors collected songs from 14 artists. The experiments in this paper utilized lyrics from the rapper Fabulous. Training data consisted of 219 verses that contained at least 175 words. 

\item \textbf{Data Preprocessing} "$<$startVerse$>$", "$<$endLine$>$" and "$<$endVerse$>$" tokens were added appropriately in the lyrics. This was useful for ultilizing non-rhyming features in the lytrics, such as syllable count and line count per verse. Additionally, the authors conjectured that the addition of these tokens would allow the models to better understand the rhyme scheme used by the artist. 

\item \textbf{LSTM Model:} 
The main appeal for LSTM networks in comparison to a traditional RNN is that they are capable of learning long-term dependencies. When the gap between the relevant information and the place that it is needed is small, RNNs can learn to use the past information. However, when this gap increases, RNNs become unable to leverage past information. 

The LSTM model archiecture is similar to that of a traditional RNN in that contains a chain of repeating modules. In an RNN, the repeating module often had a similar architecture, for example, a tanh layer. The repeating module of the LSTM is completely different. Figure 1 shows the LSTM achitecture in further detail.

\includegraphics[scale=.7]{images/LSTM-diagram.png}

\centering \caption{Figure 1: LSTM Repeating Module}

\item \textbf{N-gram Model:}  
An n-gram model is designed to learn in collections of n words from the corpus. Given that $[w_{k+n-1},..., w_{k}]$ is the history of words, this system generates a new token $t$ using the following definition:
$\frac{[w_{k+n-1},..., w_{k}], t}{[w_{k+n-1},..., w_{k}], *}$]. $[w_{k+n-1},..., w_{k}], t$ is the amount of times the context $[w_{k+n-1},..., w_{k}]$ is followed by $t$ in the training data, and $[w_{k+n-1},..., w_{k}], *$ is the number of times the context appears followed by any token. 

\end{item}

\section{Results}
The models were analyzed using two different metrics: similarity of existing lyrics and rhyme density. This is due to the in-feasibility of using an team of human annotators to judge the quality of generated lyrics, as they would have to be well versed in the artist's music. 

\textbf{Similarity:} The verses generated by the model were compared to the training verses by first converting each verse into a Term-Document matrix and then using the cosine similarity formula. The lower the similarity score, the more original the generated lyrics are. 

\textbf{Rhyme density:} Rhyme density was defined as the total number of rhymed syllables divided by the total number of syllables. These two metrics helped analyze how novel the generated lyrics are while also making sure that they are similar in structure to the artist's lyrics. 

The LSTM model proved to be more effective at ghostwriting than the n-gram model, as it achieved a similarity score of 0.41 and a rhyme density score of 0.35, which is very similar to the rapper Fabulous' rhyme density score of 0.34. Figures 1 and 2 in the paper illustrate the correlation between the rhyme density and similarity for both models. The n-gram models was able to achieve low similarities for certain generated verses, but it was at the cost of an extremely high similarity score.

\section{Conclusion and Future Works}
This paper compared the performance of the LSTM model to an n-gram model for the task of authentic ghostwriting. The LSTM model was able to produce better lyrics that also reflected the style of the artist. Future works include using more training data to allow the model to more accurately identify rhyming pairs and use them in generated verses. In addition, another experiment will be performed that uses an artist with a broader range of vocabulary to determine if the model is more effective for artists who have a small vocabulary. To improve the metrics, methods that evaluate the fluency of generated lyrics will be incorporated. 

\end{document}
